\documentclass[11pt, oneside]{book}
\usepackage{geometry}
\geometry{letterpaper}
\usepackage[parfill]{parskip}
\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

% Essential Packages
\usepackage{ragged2e}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[hyperref]{ntheorem}

% Theorem Style Customization
\setlength\theorempreskipamount{2ex}
\setlength\theorempostskipamount{3ex}

% hyperref Package Settings
\usepackage{hyperref}
\hypersetup{
	colorlinks = true,
	linkcolor = magenta
}

% ntheorem Declarations
\theoremstyle{break}
\newtheorem{thm}{Theorem}[section]
\newtheorem*{proof}{Proof}
%\newtheorem{crly}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{propo}{Proposition}[section]
\newtheorem{claim}{Claim}[section]
\newtheorem*{remark}{Remark}
\newtheorem*{note}{Note}
\newtheorem{defn}{Definition}[section]
\newtheorem{eg}{Example}[section]
\newtheorem{ex}{Exercise}[section]
\newtheorem{cor}{Corollary}[section]

\newcommand{\cA}{ {\cal A} }
\newcommand{\fA}{ {\mathfrak{A}} }
\newcommand{\cB}{ {\cal B} }
\newcommand{\cD}{ {\cal D} }
\newcommand{\cF}{ {\cal F} }
\newcommand{\cL}{ {\cal L} }

\newcommand{\bA}{ {\mathbb{A}} }
\newcommand{\bC}{ {\mathbb{C}} }
\newcommand{\bF}{ {\mathbb{F}} }
\newcommand{\bN}{ {\mathbb{N}} }
\newcommand{\bQ}{ {\mathbb{Q}} }
\newcommand{\bR}{ {\mathbb{R}} }
\newcommand{\bZ}{ {\mathbb{Z}} }
\newcommand{\bP}{ {\mathbb{P}} }
\newcommand{\bo}{ {\mathcal{O}} }
\newcommand{\bbc}{ {\mathcal{C}} }


\newcommand{\ba}{ \vec{a} }
\newcommand{\bb}{ \vec{b} }
\newcommand{\pp}{ \vec{p} }
\newcommand{\qq}{ \vec{q} }
\newcommand{\ts}{ \vec{s} }
\newcommand{\uu}{ \vec{u} }
\newcommand{\vv}{ \vec{v} }
\newcommand{\xx}{ \vec{x} }
\newcommand{\yy}{ \vec{y} }
\newcommand{\zz}{ \vec{z} }
\newcommand{\zv}{ \vec{0} }

\newcommand{\aks}{ ( \, \ba_k \, )_{k=1}^{\infty} }
\newcommand{\xks}{ ( \, \xx_k \, )_{k=1}^{\infty} }
\newcommand{\yks}{ ( \, \yy_k \, )_{k=1}^{\infty} }
\newcommand{\qks}{ ( q_k )_{k=1}^{\infty} }
\newcommand{\tks}{ ( t_k )_{k=1}^{\infty} }

\newcommand{\limk}{ \lim_{k \to \infty} }
\newcommand{\ee}{ \varepsilon }
\newcommand{\limi}{ \lim_{i \to \infty} }

\providecommand*{\axiomautorefname}{Axiom}
\providecommand*{\lemmaautorefname}{Lemma}
\providecommand*{\thmautorefname}{Theorem}
\providecommand*{\propoautorefname}{Proposition}

% ntheorem listtheorem style
\makeatletter
\def\thm@@thmline@name#1#2#3#4{%
        \@dottedtocline{-2}{0em}{2.3em}%
                   {\makebox[\widesttheorem][l]{#1 \protect\numberline{#2}}#3}%
                   {#4}}
\@ifpackageloaded{hyperref}{
\def\thm@@thmline@name#1#2#3#4#5{%
    \ifx\#5\%
        \@dottedtocline{-2}{0em}{2.3em}%
            {\makebox[\widesttheorem][l]{#1 \protect\numberline{#2}}#3}%
            {#4}
    \else
        \ifHy@linktocpage\relax\relax
            \@dottedtocline{-2}{0em}{2.3em}%
                {\makebox[\widesttheorem][l]{#1 \protect\numberline{#2}}#3}%
                {\hyper@linkstart{link}{#5}{#4}\hyper@linkend}%
        \else
            \@dottedtocline{-2}{0em}{2.3em}%
                {\hyper@linkstart{link}{#5}%
                  {\makebox[\widesttheorem][l]{#1 \protect\numberline{#2}}#3}\hyper@linkend}%
                    {#4}%
        \fi
    \fi}
}
\makeatother
\newlength\widesttheorem
\AtBeginDocument{
  \settowidth{\widesttheorem}{Proposition A.1.1.1\quad}
}

\theoremlisttype{allname}

% Shortcuts
%\newcommand{\bb}[1]{\mathbb{#1}}			% using bb instead of mathbb
\newcommand{\floor}[1]{\lfloor #1 \rfloor}	% simplifying the writing of a floor function
\newcommand{\ceiling}[1]{\lceil #1 \rceil}	% simplifying the writing of a ceiling function
\newcommand{\dotp}{\, \cdotp}				% dot product to distinguish from \cdot

% Custom math operator
\DeclareMathOperator{\rem}{rem}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\order}{order}

% Main Body
\title{CO 367 - Nonlinear Optimization (Class Notes)}
\author{Saiyue Lyu}

\begin{document}
\hypersetup{pageanchor=false}
\maketitle
\hypersetup{pageanchor=true}
\tableofcontents

\chapter*{List of Definitions}
\theoremlisttype{all}
\listtheorems{defn}

\chapter*{List of Theorems}
\theoremlisttype{allname}
\listtheorems{axiom,lemma,thm,crly,propo}

\chapter*{List of Symbols}\label{symbol_list}
\begin{tabular}{l l}
    $S_n$               &   symmetric group on n letters \\
    $D_{2n}$            &   dihedral group of order 2n \\
    $A \times B$        &   Cartesian product of A and B \\
    $A \simeq B$        &   A is isomorphic to B \\
    $\ker(\phi)$        &   kernel of $\phi$ \\
    $I_m(\phi)$         &   image set of $\phi$ \\
    $gH, Hg$            &   left coset, right coset of H with coset representative g \\
    $[G : H]$           &   index of the subgroup H in G \\
    $H \triangleleft G$ &   H is a normal subgroup of G \\
    $\rem_n(a)$         &   remainder of a when divided by n \\

\end{tabular}

\chapter{Introduction}

Mathematical Optimization (formally math programming)

Find a best soln to the model of a problem

{\bf Application : }

$\bullet$ Operation Research

\hspace{1cm} 1) Scheduling and Planning

\hspace{1cm} 2) Supply Chain Management

\hspace{1cm} 3) Vehicle Routing

\hspace{1cm} 4) Power Grid Optimization

$\bullet$ Statistics and Machine Learning

\hspace{1cm} 1) Curve Fitting

\hspace{1cm} 2) Classification, Clustering, SVM ...

\hspace{1cm} 3) Deep Learning

$\bullet$ Finance

$\bullet$ Optimal Control

$\bullet$ Biology -- Protein Folding

\newpage

Optimization

\begin{equation*}
\begin{aligned}
(OPT)& \underset{X}{\text{minimize}}
& & f(x) & & \mbox{ objective function}\\
& \text{subject to}
& & g_i(x)\leq 0, \; \forall i=1,\cdots,m & &\mbox{ constraints}\\
&&& x \in\bR^n .
\end{aligned}
\end{equation*}

\remark

1) $\max f(x) =-\min -f(x)$

2) $\{ x\in\bR^n, g(x)\geq 0\}=\{x\in\bR^n, -g(x)\leq 0\}$

3) $\{x\in\bR^n, g(x)\leq b\}=\{x\in\bR^n, g(x)-b\leq 0\}$

\section{Classification of Solns}
\defn[Open ball \& Closure] The open ball of radius $\delta$ around $\bar{x}$ is $B_{\delta}(\bar{x})=\{x\in\bR^n, ||x-\bar{x}||<\delta\}$ 

The closure of $B_{\delta}(\bar{x})$ is $\overline{B_{\delta}}(\bar{x})=\{x\in\bR^n, ||x-\bar{x}||\leq \delta\}$

\defn[Global \& Local Minimizer] Consider $f: D\to \bR$. the point $x^*\in D$ is 

$\bullet$ a global minimizer for $f$ on $D$ if $f(x^*)\leq f(x),\forall x\in D$

$\bullet$ a strict global minimizer for $f$ on $D$ if $f(x^*)< f(x),\forall x\in D, x\neq x^*$

$\bullet$ a local minimizer for $f$ on $D$ if $\exists \delta>0, f(x^*)\leq f(x),\forall x\in B_{\delta}(x^*)\cap D$

$\bullet$ a strict local minimizer for $f$ on $D$ if $\exists \delta>0, f(x^*)< f(x),\forall x\in B_{\delta}(x^*)\cap D, x\neq x^*$

\section{Classification of Problems}
1. If $f(x)=0, \forall x\in \bR^n$, then (OPT) is a feasible problem

2. If we have $m=0$ constraints, then (OPT) is an unconstrained optimization problem.

\section{Classification of Problems -- Types of functions involved}

Why do we care?

In the absence of hypothesis on $f$ and $g$, (OPT) is unsovlable.

\note "Black box" optimization framework.

All we have is an "oracle" that can compute values of $f(x)$ for any $x$ (and possibly some derivatives)

\eg Consider $h(x)=\begin{cases}
    0,& \text{if } x\in\bZ^n\\
    1,              & \text{otherwise}
\end{cases}$
\begin{equation*}
\begin{aligned}
& \underset{X}{\text{minimize}}
& & f(x) \\
& \text{subject to}
& & g_i(x)\leq 0, \; \forall i=1,\cdots,m \\
&&& h(x)\leq 0,\\
&&& x \in\bR^n .
\end{aligned}
\end{equation*}

In other word, we want $x\in\bZ^n$, where $\bZ^n$ is a lattice

\defn[discrete optimization problem]
When the constraints of (OPT) restrict solns to a lattice, then (OPT) is called a discrete optimization problem

\defn[Continuous Function]
A function $f: D\to \bR$ is continuous over $D$ if $\forall \epsilon>0, \exists \delta>0$ such that $|x-y|<\delta\,\Leftarrow\,|f(x)-f(y)|<\epsilon,\forall x,y\in D$

\defn[$C^k$-smooth]
A function $f:D\to\bR$, $D\subset \bR^n$ is open, then $f$ is $C^k$-smooth over $D$ \big(i.e. $f\in C^k(D)$\big) if all its $\leq k$-th derivatives are continuous over $D$

\eg  

$h(x)=\begin{cases}
    1,& \text{if } x\geq 2\\
    -1,& \text{if } x< 2            
\end{cases}$ is discontinuous

$g(x)=|x-2|$ is continuous and $C^0$ smooth

$f(x)=\begin{cases}
    \frac{1}{2}(x-2)^2,& \text{if } x\geq 2\\
    \frac{1}{@}(2-x)^2,& \text{if } x< 2            
\end{cases}$ is continuous and $C^1$ smooth

\defn[Gradient]
Let $f\in C^1(D)$ for some $D\subset \bR^n$. Its Gradient $\nabla f\in C^0(D): D\to \bR^n$ is given by 
\begin{center}
    $\nabla f(x)=\begin{bmatrix}
    \frac{\partial f}{\partial x_1}(x)\\
    \vdots
    \frac{\partial f}{\partial x_n}(x)\\
    \end{bmatrix}$
\end{center}

\defn[Hessian]
Let $f\in C^2(D)$ for some $D\subset \bR^n$. Its Hessian $\nabla^2 f\in C^1(D): D\to \bR^{n\times n}$ is given by 
\begin{center}
    $\nabla^2 f(x)=\begin{bmatrix}
    \frac{\partial f}{\partial x_1\partial x_1}(x)  & \dots  &  \frac{\partial f}{\partial x_1\partial x_n}(x)\\
    \vdots  & \ddots & \vdots \\
     \frac{\partial f}{\partial x_n\partial x_1}(x)  & \dots  &  \frac{\partial f}{\partial x_n\partial x_n}(x)
\end{bmatrix}$
\end{center}

\remark
If $f$ and $g$ are linear functions, then (OPT) is a linear programming problem.


\chapter{Linear Algebra}
\section{Vector and Matrix Norm}
\defn[Norm]
A norm $||\cdot||$ on $\bR^n$ assigns a scalar $||x||$ to every $x\in\bR^n$ such that

1) $||x||\geq 0,\forall x\in\bR^n$

2) $||c\cdot x||=|c|\cdot||x|\,\forall c\in\bR, x\in\bR^n$

3) $||x||=0\Longleftrightarrow x=0$

4) $||x+y||\leq ||x||+||y||$

\begin{remark}
\begin{align*}
    &L^k Norm & ||x||_k=(\sum_{i=1}^n|x_i|^k)^{1/k}\\
    &Manhattan Norm  & ||x||_1=\sum|x_i|\\
    &Euclidean Norm & ||x||_2=\sqrt{\sum x_i^2}\\
    &Infinite Norm & ||x||_{\infty}=\max |x_i|
\end{align*}
\end{remark}

\begin{thm}[Schwartz Inequality]
$\forall x,y\in\bR^n, |x^Ty|\leq ||x||_2\cdot||y||_2$, the equality holds when $x=\lambda y$ for some $\lambda\in\bR$
\end{thm}

\begin{thm}[Pythagorean Thm]
If $x,y\in\bR^n$ are orthogonal, then $||x+y||_2^2=||x||_2^2+||y||_2^2$
\end{thm}

\begin{defn}[Induced Norm]
Given a vector norm $||\cdot||$, the induced matrix norm associates a scalar $||A||$ to all $A\in\bR^{n \times n}$ with $||A||=\displaystyle\max_{||x||=1}||Ax||$
\end{defn}

\begin{propo}
$||A||_2=\displaystyle\max_{||x||_2=1}||Ax||_2=\displaystyle\max_{||x||_2=||y||_2=1}|y^TAx|$
\begin{proof}
Apply Schwartz Inequality to $|y^TAx|$
\end{proof}
\end{propo}

\begin{propo}
$||A||_2=||A^T||_2$
\begin{proof}
Swap $x$ and $y$ in the above Proposition 2.1.1
\end{proof}
\end{propo}

\begin{propo}
Let $A\in\bR^{n \times n}$, TFAE:

1) $A$ is nonsingular

2) $A^T$ is nonsingular

3) $\forall x\in \bR^n\setminus\{0\}, Ax\neq 0$

4) $\forall b\in\bR^n, \exists x\in\bR^n$ unique such that $Ax=b$

5) Columns of $A$ are linear independent

6) Rows of $A$ are linearly independent

7) $\exists B\in\bR^{n \times n}$ unique such that $AB=I=BA$, where $B$ is the inverse of $A$

8) $\forall A,B\in\bR^{n \times n}, (AB)^{-1}=B^{-1}A^{-1}$ if $B^{-1}$ exists
\end{propo}

\section{Eigenvalues}

\defn[Eigenvalue \& Eigenvector]
The characteristic polynomial $\phi : \bR\to\bR$ of $A\in\bR^{n \times n}$ is $\phi(\lambda)=\det(A-\lambda I)$. It has $n$ (possibly complex or repeated) roots, which are the eigenvalues of $A$. Given an eigenvalue $\lambda$ of $A$, $x\in\bR^n$ is the corresponding eigenvector of $A$ if $Ax=\lambda x$

\propo
Given $A\in\bR^{n\times n}$

1) $\lambda$ is an eigenvalue $\Longleftrightarrow$
 $\exists$ a corresponding eigenvector
 
2) $A$ is singular $\Longleftrightarrow$ it has a zero eigenvector

3) If $A$ is triangular, then its eigenvector are its diagonal entries

4) If $S\in\bR^{n\times n}$ is nonsingular and $B=SAS^{-1}$, then $A,B$ have the same eigenvalues

5) If the eigenvalues of $A$ are $\lambda_1,\cdots,\lambda_n$, then 

\hspace{0.5cm} $\bullet$ the eigenvalues of $A+cI$ are $\lambda_1+c,\cdots,\lambda_n+c$

\hspace{0.5cm} $\bullet$ the eigenvalues of $A^k$ are $\lambda_1^k,\cdots,\lambda_n^k, k\in\bR$

\hspace{0.5cm} $\bullet$ the eigenvalues of $A^{-1}$ are $\frac{1}{\lambda_1},\cdots,\frac{1}{\lambda_n}$

\hspace{0.5cm} $\bullet$ the eigenvalues of $A^T$ are $\lambda_1,\cdots,\lambda_n$

\defn[Spectral Radius]
The spectral radius of $\rho(A)$ of $A\in\bR^{n\times n}$ is $\displaystyle\max_{\lambda \mbox{ is eigenvalue}}|\lambda|$

\propo
For any induced norm $||\cdot||, \rho(A)\leq ||A^k||^{1/k}$ for $k=1,2,\cdots$
\begin{proof}
{\bf Trick : } $||A^k||=\displaystyle\max_{||y||=1}||A^ky||=\displaystyle\max_{y\neq 0}\frac{1}{||y||}||A^ky||$

In particular, let $\lambda$ be any eigenvalue of $A$, $x$ be the corresponding eigenvector
\begin{align*}
    ||A^k||\geq \frac{1}{||x||}||A^k\cdot x||&=\frac{1}{||x||}||A\cdots A\cdot x||\\
    &=\frac{1}{||x||}||\lambda^k \cdot x||\\
    &=|\lambda^k|
\end{align*}

So for any eigenvalue $\lambda, ||A^k||\geq |\lambda|^k$

Therefore $||A^k||^{1/k}\geq |\lambda|, \forall \lambda$, thus $||A^k||^{1/k}\geq \rho(A)$
\end{proof}

\propo
For any induced norm $||\cdot||, \displaystyle\lim_{k\to\infty}||A^k||^{1/k}=\rho(A)$
\proof Too long, omitted


\section{Symmetric Matrices}

\propo
Let $A\in\bR^{n\times n}$ be symmetric, then

1) Its eigenvalues are all $\bR$eal

2) Its eigenvetors are $n$ mutually orthogonal $\bR$eal nonzero vectors

3) If the $n$ eigenvectors $x_1,\cdots,x_n\in\bR^n$ are normalized such that $||x||_2=1$ with corresponding eigenvalues $\lambda_1,\cdots,\lambda_n$, then $A=\sum_{i=1}^n\lambda_i x_i x_i^T$
\proof  Easy

\propo
Let $A\in\bR^{n\times n}$ be symmetric, then $||A||_2=\rho(A)$
\proof
We already know $\rho(A)\leq ||A^k||^{1/k}$, in particular, $\rho(A)\leq ||A||_2$

It remains to show that $\rho(A)\geq ||A||_2$

Because the eigenvectors $x_i, i=1,\cdots,n$ of $A$ can be assumed ,utually orthogonal

Then we can write any $y\in\bR^n$ as $y=\sum_{i=1}^n \beta_i x_i$ for some $\beta_i\in\bR$

By Pythagorean Thm, $||y||_2^2=\sum\beta_i^2||x_i||_2^2$

Now $Ay=A\sum\beta_i x_i=\sum\beta_iA x_i=\sum\beta_i\lambda_ix_i$

Since all $x_i$ are mutually orthogonal, by Pthahorean Thm again, have
\begin{align*}
    ||Ay||_2^2&=\sum\beta_i^2\lambda_i^2||x_i||_2^2\\
    &\leq \sum\beta_i^2\rho(A)^2||x_i||_2^2\\
    &=\rho(A)^2||y||_2^2
\end{align*}

By which we get, $||Ay||_2\leq \rho(A)||y||_2$

Also by the definition, we have
\begin{align*}
    ||A||_2&=\displaystyle\max_{y\neq 0}\frac{1}{||y||_2}||Ay||_2\\
    &\leq \displaystyle\max_{y\neq 0}\frac{1}{||y||_2}\cdot\rho(A)||y||_2\\
    &=\rho(A)
\end{align*}

\begin{propo}
Let $A\in\bR^{n\times n}$ be symmetric with eigenvalues $\lambda_1,\cdots,\lambda_n\in\bR$

Then $\forall y\in\bR^n, \lambda_1||y||_2^2\leq y^TAy\leq \lambda_n||y||_2^2$
\begin{proof}
Again, write $y=\sum \beta_i x_i$ for some $\beta_i\in\bR$ with $x_i$ are the orthogonal eigenvectors

On the one hand,
\begin{align*}
    y^TAy&=(\sum\beta_i x_i)^T(\sum\beta_i \lambda_ix_i)\\
    &=\sum\beta_i^2\lambda_ix_i^Tx_i\mbox{ as }x_ix_j=0\mbox{ if }i\neq j\\
    &=\sum\beta_i^2\lambda_i||x_i||_2^2
\end{align*}

WLOG, we can assume $||x_i||_2=1$, then we have 
\begin{align*}
    y^TAy=\sum\beta_i^2\lambda_i
\end{align*}

On the other hand,
\begin{align*}
    ||y||_2^2=\sum\beta_i^2||x_i||_2^2=\sum\beta_i^2
\end{align*}

Clearly, we have 
\begin{align*}
    \lambda_1\sum\beta_i& &\leq& & \sum\beta_i^2\lambda_i& &\leq & & \lambda_n\sum\beta_i^2\\
    \lambda_1||y||_2^2& &\leq&  &y^TAy & &\leq &&\lambda_n||y||_2^2
\end{align*}
\end{proof}

\remark {\bf Why can we assume $||x_i||_1=1$? }

As $x_i$ being the eigenvectors of $A$ are defined up to scalar $Ax=\lambda x$, we have 
\begin{center}
 $A(\frac{1}{||x||}x)=\lambda(\frac{1}{||x||}x)$   
\end{center}
\end{propo}

\propo
Let $A\in\bR^{n\times n}$ be symmetric, then $||A^k||_2=||A||_2^k,\forall k=1,2,\cdots$
\begin{proof}
Since $A^T=A$, then $(A^k)^T=(A\cdots A)^T=A^T\cdots A^T=A\cdots A=A^k$

Since $A^k$ is symmetric, then $||A^k||_2=\rho(A^k)$

We know that the eigenvalues of $A^k$ are $\lambda_1^k,\cdots,\lambda_n^k$

Thus $\rho(A^k)=\big(\rho(A)\big)^k$

We know $\rho(A)=||A||_2$, therefore $||A||_2^k=||A^k||_2$
\end{proof}

\propo 
Let $A\in \bR^{n\times n}$ (not necessary symmetric), then $||A||_2^2=||A^TA||_2=||AA^T||_2$
\begin{proof}

On the one hand,

$||Ax||_2^2=(Ax)^T(Ax)=x^T(A^TAx)\leq ||x||_2\cdot||A^TAx||_2\leq ||x||_2\cdot||AA^T||_2\cdot||x||_2$

$||A||_2^2=\displaystyle\max_{x\in\bR^n} \frac{1}{||x||_2^2}\cdot||Ax||_2^2\leq||A^TA||_2$

On the other hand,
\begin{align*}
||A^TA||&=\displaystyle\max_{||x||=||y||=1} |y^T A^T Ax|\\
    &\leq\displaystyle\max_{||y||=1,||x||=1} ||y^TA^T||_2\cdot||Ax||_2\mbox{ by CS ineq}\\
    &=\big(\displaystyle\max_{||y||=1} ||y^TA^T||_2\big)\big(\displaystyle\max_{||x||=1}||Ax||_2\big)\\
    &=||A||_2\cdot||A||_2\\
    &=||A||_2^2
\end{align*}

Combine these two things, we get $||A||_2^2=||A^TA||_2$

For the other equality, swap $A$ and $A^T$ in the proof and use $||A||_2=||A^T||_2$
\end{proof}

\propo

$||A^{-1}||_2$ is $\frac{1}{|\lambda_1|}$, where $\lambda_1$ is the smallest magnitude eigenvalue of $A$
\begin{proof}
We know $||A^{-1}||_2=\rho(A^{-1})$, and the eigenvalues of $A^{-1}$ are the inverse of the eigenvalues of $A$
\end{proof}

\section{Positive Definite Matrices}
\defn[Positive Definite \& Positive Semidefinite]
A symmetric matrix $A\in\bR^{n\times n}$ is positive definite if $x^TAx>0,\forall x\in\bR^n, x\neq 0$, it is positive semidefinite if $x^TAx\geq 0,\forall x\in\bR^n$

\note 

pd. for symmetric positive definite, psd. for symmetric positive semidefinite

\propo
For any $A\in\bR^{n\times n}$(possibly non square), $A^TA$ is psd. Then the matrix $A^TA$ is pd iff $A$ has full column rank (i.e. $rank(A)=n$; which implies $m\geq n$)

\begin{proof}
1) $A^TA$ is square and symmetric (immediate)

2) $A^TA$ is psd. : $\forall x\in\bR^n, x^T(A^TA)x=(Ax^T)(Ax)=||Ax||_2^2\geq 0$

3) pd. iff $rank(A)=n$ : 
\begin{align*}
    &x^TA^TAx>0,\forall x\in bE^n, x\neq 0\\
    \Longleftrightarrow &||Ax||_2^2>0\\
    \Longleftrightarrow &||Ax||_2>0\\
    \Longleftrightarrow &Ax\neq 0, \forall x\in\bR^n, x\neq 0\\
    \Longleftrightarrow &\mbox{ the columns of $A$ are linearly independent}\\
    \Longleftrightarrow &rank(A)=n
\end{align*}
\end{proof}

\cor 
If $A\in\bR^{n\times n}$ is square, then $A^TA$ is pd. iff $A$ is nonsingular

\propo 
A square symmetric matrix is psd. (rsp pd.) iff all its eigenvalues are $\geq 0$ (rsp $>0$)
\begin{proof}
We will prove the statement for psd/$\geq 0$, the proof is similar for pd/(>0)

($\Rightarrow$) Let  $\lambda$ be an eigenvalue of $A$ psd. and let $x$ be the corresponding nonzero eigenvector

\hspace{0.5cm}
Then $x^tAx\geq 0$, so $x^T\lambda x=\lambda||x||_2^2\geq 0$

\hspace{0.5cm}
Thus $\lambda\geq 0$

($\Leftarrow$) Let $\lambda_1,\cdots,\lambda_n$ be the eigenvalues of $A$ and let $x_1,\cdots,x_n$ be the n (nonzero, real, mutually orthogonal) eigenvectors.

\hspace{0.5cm}
For any $y\in\bR^n$, we can write 
\begin{center}
    $y=\sum \beta_i x_i$ for some $\beta_i\in\bR$
\end{center}

\hspace{0.5cm}
Then we have 
\begin{align*}
    y^TAy&=(\sum \beta_i x_i)^T\cdot A\cdot (\sum \beta_i x_i)\\
    &=(\sum \beta_i x_i)^T(\sum\beta_i Ax_i)\\
    &=(\sum \beta_i x_i)^T(\sum \beta_i \lambda_ix_i)\\
    &=\sum\beta_i^2\lambda_i||x_i||_2^2\mbox{ as $x_i$ are orthogonal}\\
    &\geq 0
\end{align*}
\end{proof}

\propo 
The inverse of a pd. matrix is pd.
\begin{proof}
Let $\lambda_1,\cdots,\lambda_n>0$ be the eigenvalues of $A$ pd. 

Then the eigenvalues of $A^{-1}$ are $\frac{1}{\lambda_1},\cdots,\frac{1}{\lambda_n}$
\end{proof}

\chapter{Convexity}

\section{Basic Intro}

\defn[Convex Set]
A set $C\subset \bR^n$ is convex if $\lambda x+(1-\lambda)y\in C,\forall x,y\in C, \forall 0\leq \lambda\leq 1$

\eg The set of two disjoint sets is nonconvex

A "donut" is nonconvex

\defn[Convex Function]
Let $D\subset\bR^n$ be a convex set, a function $f : D\to \bR$ is sid to be convex if $f\big(\lambda x+(1-\lambda)y\big)\leq \lambda f(x)+(1-\lambda)f(y), \forall x,y\in D, \forall 0\leq \lambda\leq 1$

A function is said to be strict convex if a strict inequality ($<$) holds as well

\eg $y=x^2$ is a convex function,  \hspace{1cm} $y=-x^2$ is non-convex (concave)

\propo 
1) For any collection of $\{C_i :i\in I\}$ of convex sets, their intersection $\displaystyle\cap_{i\in I} C_i$ is convex

2) The vector (Minkowski) sum $\{x+y : x\in C_1, y\in C_2\}$ of two convex sets $C_1, C_2$ is convex

3) The image of a convex set under a linear transformation is a convex set

\defn[Level Set \& Epigraph]
Let $f: D\to \bR$ be a function with $D$ convex, 

The level sets of $f$ are $\{x\in D: f(x)\leq\alpha\}$ for all $\alpha\in\bR$ (sometimes $"<"$)

The epigraph of $f$ is s subset of $\bR^{n+1}$ given by $epi(f)=\{(x,\alpha), x\in D, \alpha\in\bR,f(x)\leq \alpha\}$

\propo
1) If $f: D\to\bR$ is convex, then its level sets are convex as well

2) $f:D\to\bR$ is convex iff its epigraph is a convex set

\note The converse of 1) is not true ! For example, $f(x)=\sqrt{|x|}$

The level sets of $f$ is $\sqrt{|x|}\leq\alpha\Longleftrightarrow|x|\leq\alpha^2\Longleftrightarrow\alpha^2\leq x\leq \alpha^2$

However, $f$ is not convex !

\propo
1) Any linear function is convex (but not strictly convex)

2) If $f$ is a convex function, then $g(x)=\lambda f(x)$ is convex for all $\lambda\geq 0$

3) The sum of two convex functions is a convex function

4) The maximum of two convex functions is a convex function (does not work for minimum)

\propo
Any vector norm is convex (this is useful as optimize convex function is usually possible)
\begin{proof}
Let $f(x)=||x||$, then $\forall x,y\in\bR^n,0\leq \lambda\leq 1$, have 
\begin{align*}
    f\big(\lambda x+(1-\lambda)y\big)&=||\lambda x+(1-\lambda)y||\\
    &\leq ||\lambda x||+||(1-\lambda)y||\\
    &=\lambda\cdot ||x||+(1-\lambda)\cdot ||y||\\
    &=\lambda f(x)+(1-\lambda) f(y)
\end{align*}
\end{proof}

\section{Taloy's Thms}

\thm[Talor's Thm For Uni-variate Functions]
$f(x+h)=\sum_{i=0}^k\frac{h^i}{i!}d_i(f)$, where $d_i(f)$ is the i-th derivative of $f$ and $\phi(x)=\frac{h^{k+1}}{(k+1)!}d_{i+1}f(x+\lambda h), 0\leq \lambda\leq 1$ is the residual function. In particular, $\lim_{h\to 0} \frac{\phi(h)}{h^k}=0$

\thm[Talor's Thm For Multivariate Functions -- 1st order
($k=1$)]
$f(x+h)=f(x)+h^T\nabla f(x)+\phi(h)$, where $\phi(h)=\frac{1}{2}h^T\nabla^2 f(x+\lambda h)h, 0\leq \lambda\leq 1$ with $\lim_{h\to 0} \frac{\phi(h)}{||h||}=0$

\thm[Talor's Thm For Multivariate Functions -- 2nd order
($k=2$)]
$f(x+h)=f(x)+h^T\nabla f(x)+\frac{1}{2} h^T\nabla^2 f(x)h+\phi(h)$ with $\lim_{h\to 0}\frac{\phi(h)}{||h||^2}=0$

\thm[Mean Value Thm]
Let $f: D\to \bR, D\subset \bR, f\in C^1(D)$, then $\forall x,y\in D, \exists z\in[x,y]$ such that $f(y)=f(x)+\nabla f(z)(y-x)$
\proof By 0-th order Talor Expansion

\defn[Directional Derivative]
The directional Derivative of $f$ in the direction of $y$ is $\nabla_y f(x)=\lim_{\alpha\to 0}\frac{f(x+\alpha y)-f(x)}{\alpha}$

In particular, $\nabla_{e_i} f(x)=\frac{\partial f}{\partial x_i}(x)$ and $\nabla f=\big(\nabla_{e_1}f(x)\cdots\nabla_{e_n}f(x)\big)^T$ 

The "direction" draws out the function

\thm
Let $f\in C^1$, then $\nabla_h f=h^T \nabla f$
\begin{proof}
\begin{align*}
\nabla_h  f&=\lim_{\alpha\to 0}\frac{f(x+\alpha h)-f(x0}{\alpha}\\
&=\lim_{\alpha\to 0}\frac{f(x)+\alpha h^T\nabla f(x)+\phi(\alpha h)-f(x)}{\alpha}\\
&=\lim_{\alpha\to 0}\frac{\alpha h^T\nabla f(x)+\phi(\alpha h)}{\alpha}\\
&=\lim_{\alpha \to 0}\frac{\alpha h^T \nabla f(x)}{\alpha}+\lim_{\alpha\to 0}\frac{\phi(\alpha h)}{\alpha}\\
&=h^T\nabla f(x)+\lim_{\alpha\to 0} \frac{\phi(\alpha h)}{\alpha}\\
&=h^T\nabla f(x)\mbox{ by definition of residual above}
\end{align*}
\end{proof}

\propo
Let $D\subset\bR^n$ be convex and $f:D\to\bR$ be differentiable over $D$. Then $f$ is convex iff $f(z)\geq f(x)+(z-x)^T\nabla f(x),\forall x,z\in D$
\begin{figure}[ht!]
	\centering
	\includegraphics[width=12cm, height=3.5cm]{321}
\end{figure}
\proof
$(\Longrightarrow)$ As $D$ is convex, then $x+(z-x)\alpha =\alpha z+(1-\alpha)x\in D, \forall 0\leq \alpha\leq 1$
\begin{align*}
    \lim_{\alpha\to 0}\frac{f\big(x+\alpha(z-x)\big)-f(x)}{\alpha}=\nabla_{z-x}f(x)=(z-x)^Tf(x)
\end{align*}
By convexity of f, $\forall 0\leq \alpha\leq 1$
\begin{align*}
f\big(x+\alpha(z-x)\big)&\leq \alpha f(z)+(1-\alpha)f(x)\\
f\big(x+\alpha(z-x)\big)-f(x)&\leq \alpha f(z)-\alpha f(x)\\
\frac{f\big(x+\alpha(z-x)\big)-f(x)}{\alpha}&\leq f(z)-f(x)
\end{align*}
Taking the $\lim_{\alpha\to 0}$
\begin{align*}
    (z-x)^T\nabla f(x)\leq f(z)-f(x)
\end{align*}

$(\Longleftarrow)$ If $f(z)\geq f(x)+(z-x)^T\nabla f(x), \forall x,z\in D$

Let $a,b\in D$ be any points in the domain of $f$, let $c:=\alpha a+(1-\alpha )b$
\begin{align}
    f(a)\geq f(c)+(a-c)^T\nabla f(x)\\
    f(b)\geq f(c)+(b-c)^T\nabla f(x)
\end{align}

Multiply (3.1) by $\alpha$ and (3.2) by $(1-\alpha)$, then add them together, we get:
\begin{align*}
    \alpha f(a)+(1-\alpha)f(b)&\geq \alpha\big(f(c)+(a-c)^T\nabla f(c)\big)+(1-\alpha)\big(f(c)+(b-c)^T\nabla f(c)\big)\\
    &\geq f(c)+\alpha(a-c)^T\nabla f(c)+(1-\alpha)(b-c)^T\nabla f(c)\\
    &\geq f(c)+(\alpha a-\alpha c+b-\alpha b-c+\alpha c)^T\nabla f(c)\\
    &\geq f(c)+(\alpha a+b-\alpha b-c)^T\nabla f(c)\\
    &\geq f(c)\\
    &\geq f\big(\alpha a+(1-\alpha )b\big)
\end{align*}

Hence $f$ is convec over $D$

\propo
Let $f:\bR^n\to\bR, f\in C^2(D)$, then

(1) If $\nabla^2f(x), \forall x\in D$ is p.s.d., then $f$ is convex over $D$

(2) If $\nabla^2f(x), \forall x\in D$ is p.d., then $f$ is strict convex over $D$

(3) If $D=\bR^n$ and $f$ is convex over $\bR^n$, then $\nabla^2f(x), \forall x\in D$ is p.s.d.

\begin{proof}
(1) $\forall x,y\in D$, by 1st order Taylor
\begin{align*}
    f(y)=f(x)+(y-x)^T\nabla f(x)+\frac{1}{2}(y-x)^T\nabla^2 f\big(x+\alpha (y-x)\big)(y-x), 0\leq \alpha\leq 1
\end{align*}
(2) Similar to (1) with $y\neq x$, strict inequality

(3) Suppose for contradiction that $\exists x,z\in\bR^n$ such that $z^T\nabla^2f(x)z<0$

Since $\nabla^2f(x)$ is continuous, we can find a $z$ small enough that $z^T\nabla^2f(x+\alpha z)z<0, \forall 0\leq \alpha \leq 1$

By Taylor
\begin{align*}
    f(x+z)&=x(x)+z^T\nabla f(x)+\frac{1}{2}z^T\nabla^2f(x+\beta z)z, 0\leq \beta\leq 1\\
    &<f(x)+z^T\nabla f(x)
\end{align*}

Which contradicts convexity
\end{proof}

\chapter{Optimality Conditions}
\defn[Critical/Stationary Points]
All $x$ such that $\nabla f(x)=0$ are called critical or stationary points

All local minimizers are critical points, but the converse is not always true

\note 
$\nabla^2 f$ is symmetric since 
\begin{center}
$\frac{\partial^2 f}{\partial x_i\partial x_j}=\frac{\partial^2 f}{\partial x_j\partial x_i}$
\end{center}

\thm[First Order Necessary Conditions For Optimality]
Let $f:\bR^n\to \bR$ be $C^1$-smooth. If $x^*$ is a local minimizer, then $\nabla f(x^*)=0$
\begin{proof}
Let $B_{\delta}(x^*)$ be such that $f(x^*)\leq f(x), \forall x\in B_{\delta}(x^*)$

$\forall i, \forall |h|<\delta, f(x^*+h\cdot e_i)-f(x^*)\geq 0$

Hence $\frac{f(x^*+h\cdot e_i)-f(x^*)}{h}\geq 0$ if $h>0$

\hspace{1cm}$\frac{f(x^*+h\cdot e_i)-f(x^*)}{h}\leq 0$ if $h<0$

Since $f\in C^1$, then $\lim_{h\to 0}\frac{f(x^*+h\cdot e_i)}{h}$ exists

If both $\geq 0, \leq 0$ hold, then $=0$ hold

Hence $\frac{\partial f}{\partial x_i}(x^*)=0, \forall i$

Therefore $\nabla f(x^*)=0$
\end{proof}

\thm[Second Order Necessary Conditions For Local Optimality]
Let $f: \bR^n\to \bR$ be $C^2$-smooth. If $x^*$ is a local minimizer, then $\nabla f(x^*)=0$ and $\nabla ^2 f(x^*)$ is p.s.d.
\begin{proof}
Let $z\in\bR^n\backslash \{0\}$, we need to prove $z^T\nabla^2 f(x^*)z\geq 0$

Let $B_{\delta}(x^*)$ be such that $f(x^*)\leq f(x),\forall x\in B_{\delta}(x^*)$

Let $y:=h\cdot \frac{z}{||z||}$ with $0< h<\delta$, then we have 
\begin{align*}
    f(x^*+y)-f(x^*)&\geq 0\\
    f(x^*)+y^T\nabla f(x^*)+\frac{1}{2}y^T\nabla^2f(x^*)y+\phi(x)-f(x^*)&\geq 0
    \mbox{ where }\lim_{y\to 0, y\neq 0}\frac{\phi(y)}{||y||}=0
\end{align*}

By 1st order condition, we have $y^T\nabla f(x^*)=0$, hence we have 
\begin{align*}
    \frac{1}{2}\frac{h^2}{||z||^2} z^T\nabla^2f(x^*)z+\phi(h\frac{z}{||z||})&\geq 0\\
    z^T\nabla^2f(x^*)z+2||z||^2\frac{1}{h^2}\phi(h \frac{z}{||z||})&\geq 0
\end{align*}

Take the limit when $h\to 0$, by Talor, we have
\begin{align*}
    \lim_{h\to 0, h\neq 0}\frac{\phi(h\cdot \frac{z}{||z||})}{h^2}=0
\end{align*}

Therefore we have $z^T\nabla^2f(x^*)z\geq 0$
\end{proof}

\thm[Second Order Sufficient Conditions For Local Optimality]
Let $f:\bR^n\to\bR\in C^2\big(B_{\delta}(x^*)\big), x^*\in\bR, \delta>0$. If $\nabla f(x^*)=0$ and $\nabla^2f(x^*)$ is p.d., then $x^*$ is a strict local minimizer
\begin{proof}
By Talor 2nd order, $\forall h\in B_{\delta}(x^*)$
\begin{center}
    $f(x^*)+h^T\nabla f(x^*)+\frac{1}{2}h^T\nabla^2f(x^*)h+\phi(x)$ where $\lim_{h\to 0, h\neq 0}\frac{\phi(h)}{||h||}=0$
\end{center}

Let $0<\lambda_1<\cdots<\lambda_n$ be the positive eigenvalues of $\nabla^2f(x^*)$

By the definition of limit
\begin{align*}
    \exists r>0 : \forall h\in B_r(x^*), |\frac{\phi(x)}{||h||^2}|\leq \frac{\lambda_1}{4}\,\,\,\,\,\Longleftrightarrow\,\,\,|\phi(x)|\leq ||h||^2\frac{\lambda_1}{4}
\end{align*}
Remember that, 
\begin{align*}
    ||y||^2\cdot \lambda_1\leq y^T\nabla^2f(x^*)y\leq ||y||^2\cdot \lambda_n
\end{align*}

Also by assumption, $\nabla f(x^*)=0$, then we have
\begin{align*}
    f(x^*+h)&=f(x^*)+\frac{1}{2}h^T\nabla^2 f(x^*)h+\phi(h)\\
    &\geq f(x^*)+\frac{1}{2}||h||^2\lambda_1-||h||^2\frac{\lambda_1}{4}\\
    &=f(x^*)+\frac{1}{4}||h||^2\cdot\lambda_1\\
    &> f(x^*)\mbox{ for all }h\in B_r(x^*)\backslash\{0\} 
\end{align*}

Therefore $x^*$ is a strict local minimizer over $B_r(x^*)$
\end{proof}

\section{Summary For Necessary And Sufficient Optimality Conditions}
\[\begin{cases}
    \nabla f(x^*)=0\\
    \nabla^2 f(x^*)\,\,p.d.
\end{cases}\]

$\xrightarrow{\text (1)}$ 
\begin{center}
$x^*$ is a strict local minimizer
\end{center}

$\xrightarrow{\text (2)}$ 
\begin{center}
$x^*$ is a local minimizer
\end{center}
$\xrightarrow{\text (3)}$ 
\[\begin{cases}
    \nabla f(x^*)=0\\
    \nabla^2 f(x^*) \,\,\,p.s.d.
\end{cases}\]

The converses of (1), (2), (3) are all false!!!

Counterexamples:

(1) $f(x)=x^4$ at $x^*=0$

(2) $f(x)=1$ at $x^*=0$

(3) $f(x)=x^3$ at $x^*=0$

\thm
Let $C\subset \bR^n$ be a convex set, and $f: C\to \bR$ be a convex function. A local minimizer of $f$ is also a global minimizer. If $f$ is strictly convex, then there is at most one global minimizer
\begin{proof}
Suppose $x^*$ is a local minimizer, and $y^*$ is a global minimizer with $f(y^*)\leq f(x^*)$

By convexity of $f$, have 
\begin{align*}
    f\big(\alpha y^*+(1-\alpha)x^*\big)&\leq \alpha \cdot f(y^*)+(1-\alpha)\cdot f(x^*)\\
    &=f(x^*)+\alpha\cdot\big(f(y^*)-f(x^*)\big)\\
    &<f(x^*), \forall 0\leq \alpha\leq 1
\end{align*}

Thus, $\forall r>0, \exists z\neq x^*$ such that $||z-x^*||<r$ and $f(z)<f(x^*)$

For instance, $z=\alpha\cdot y^*+(1-\alpha)\cdot x^*$ with $\alpha=\frac{r}{2}||y^*-x^*||$

Thus $x^*$ is not a local minimizer, which is a contradiction

Therefore $f(y^*)\geq f(x^*)$
\end{proof}

\section{P.S.D}

\thm[Spectral Decomposition of Symmetric P.S.D.]
$\forall A\in\bR^{n\times n}$ symmetric, $\exists D,Q\in\bR^{n\times n}$ such that

(1) $D$ is Diagonal, its diagonal entries are eigenvalues of $A$

(2) $Q$ is orthogonal, i.e. $Q^{-1}=Q^T$

(3) $A=QDQ^T$
\begin{proof}
Let $\lambda_1,\cdots,\lambda_n$ be the eigenvalues of $A$ and $x_1,\cdots,x_n$ be their corresponding eigenvectors

Then $\forall i=1,\cdots,n, Ax_i=\lambda_ix_i$, thus
\begin{align*}
    A\cdot\begin{bmatrix}
    x_1\\x_2\\\vdots\\x_n\end{bmatrix}^T=
    \begin{bmatrix}
    \lambda_1x_1\\\lambda_2x_2\\\vdots\\\lambda_nx_n\end{bmatrix}^T=
    \begin{bmatrix}
    x_1\\x_2\\\vdots\\x_n\end{bmatrix}^T
    \cdot
    \begin{bmatrix}
    \lambda_1 & 0 & 0 & \dots & 0\\
    0 & \lambda_2 & 0 & \dots & 0\\
    0 & 0 & \lambda_3 & \dots & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & 0 & \dots & \lambda_n
    \end{bmatrix}
    =\begin{bmatrix}
    x_1\\x_2\\\vdots\\x_n\end{bmatrix}^T
    \cdot diag (\lambda_1,\cdots,\lambda_n)
\end{align*}

As $A$ is symmetric, these $x_i$'s are mutually orthogonal, i.e. $x_ix_j=0$ when $i\neq j$

WLOG, assume $||x_i||=1$

Let $Q=
\begin{bmatrix}
x_1 & x_2 &\dots &x_n
\end{bmatrix}$, then $Q^TQ=I$

Let $D=diag(\lambda_1,\cdots,\lambda_n)$

We get $AQ=QD$, thus $A=QDQ^{-1}=QDQ^T$
\end{proof}

\thm[Cholestic Decomposition]
Let $A\in\bR^{n\times n}$ be symmetric, then

$A$ is p.s.d. $\Longleftrightarrow$ $\exists \,G\in \bR^{n\times n}$ such that $A=GG^T$
\begin{proof}
($\Longrightarrow$) Assume $A=QDQ^T$ by the previous thm, where $Q^T=Q^{-1}$ and $D$ is diagonal

Denote $\sqrt{D}=diag (\sqrt{D_{11}},\cdots,\sqrt{D_{nn}})$

Let $G=Q\cdot \sqrt{D}$

Then $GG^T=Q\sqrt{D}(Q\sqrt{D})^T=Q\sqrt{D}\sqrt{D}^TQ^T=QDQ^T=A$

($\Longleftarrow$) Assume $A=GG^T$

Note that $\forall M\in \bR^{n\times n}, M^TM$ is p.s.d.

Let $M=G^T$

Observations :

(1) If $\sqrt{D}=\begin{bmatrix}
d & 0\\
0 & 0
\end{bmatrix}$, then $G=Q\sqrt{D}=\begin{bmatrix}
Q_{11} & Q_{12}\\
Q_{21} & Q_{22}
\end{bmatrix}\cdot \begin{bmatrix}
d & 0\\
0 & 0
\end{bmatrix}=\begin{bmatrix}
Q_{11} d & 0\\
Q_{21} d & 0
\end{bmatrix}
$

So $\bar{G}=\begin{bmatrix}
Q_{11}d\\Q_{21}d
\end{bmatrix}$ satisfies $\bar{G}\bar{G}^T=A$

(2) If $A$ is p.d., then $\sqrt{D}$ is invertible

Since $Q$ is always invertible, we get $G+Q\sqrt{D}\in\bR^{n\times n}$ is also invertible
\end{proof}

$\ $

\defn[Bounded Set \& Closed Set \& Compact Set]
(1) A set $S\subset \bR^n$ is bounded if $S\subset B_{\delta}(0)$ for some $\delta$ finite

(2) A set $S\subset \bR^n$ is closed if for any sequence $x_1,x_2,\cdots\in S$ such that $\lim_{i\to\infty} x_i$ exists, then $\lim_{i\to \infty}x_i\in S$

(3) A set is compact if it is bounded and closed

\thm[Existence of A Global Minimizer]
If $S\subset \bR^n$ is nonempty and compact and $f:S\to\bR$ is continuous, then $\exists \,y,z\in S $ such that $f(y)\leq f(x)\leq f(z),\forall x\in S$

\thm[Continuous Leads To Closed Level Set]
If $f$ is continuous, then its level sets are closed
\begin{proof}
Let $S=\{x\in\bR^n:f(x)\leq \alpha\}$ be any level set

For any sequence $x_1,x_2,\cdots\in S$, we have $f(x_i)\leq \alpha$. then by the continuity of $f$
\begin{align*}
    f(\lim_{i\to\infty} x_i)=\lim_{i\to\infty}f(x_i)\leq \alpha
\end{align*}
Thus $\lim_{i\to\infty}x_i\in S$
\end{proof}

\thm[Continuous And Bounded Level Set Gives Global Minimizer]
If $f:\bR^n\to \bR$ is continuous and has at least one bounded nonempty level set, then $f$ has a global minimizer
\begin{proof}
Let $\alpha$ be such that $S=\{x\in\bR^n:f(x)\leq \alpha\}$ is bounded nonempty

By Thm 4.2.4, $S$ is closed, thus compact

By Thm 4.2.3, $f$ has a "global" minimizer over $S$ :
\begin{align*}
    \exists\,y\in S :\,f(y)\leq f(x),\forall x\in S
\end{align*}

Consider all points $x\in\bR^n\backslash\{S\}$, we have $f(x)>\alpha\geq f(y)$

Thus $f(y)\leq f(x), \forall x\in\bR^n$
\end{proof}

\eg[Functions without global minimizers]
Want to show each level set is either unbounded or empty

(1) $f(x)=2x$, pick any $\alpha$, see that the level set is unbounded

(2) $f(x)=e^x$, if $\alpha=2$, then $S=\{x\in\bR : x\leq \ln 2\}$, if $\alpha<0$, then $S=\emptyset$

\defn[Coercive Function]
A function $f:\bR^n\to \bR$ is coercive if all its level sets are bounded

\note
If $f$ is coercive, unless $f(x)=\pm\infty, \forall x$, then it has a global minimizer

\thm[Equivalence of Coercive]
Let $f:\bR^n\to\bR$ be a continuous function, TFAE:

(1) $f$ is coercive

(2) $\forall r\in \bR, \exists\,m>0$ such that $||x||\geq m\,\Rightarrow \,f(x)\geq r$ (If we want $f$ above $r$, $x$ has to be $m-$far away from origin

\begin{proof}
$(2)\Rightarrow (1)$ Consider $S=\{x\in\bR^n:f(x)\leq \alpha\}$

\hspace{1.6cm}By (2), let $r=\alpha+1$, we have 
\begin{align*}
    \exists\,m>0\,:\,||x||\geq m\,\Rightarrow\,f(x)\geq \alpha+1
\end{align*}

\hspace{1.6cm}So $S\subset B_m(0)$, i.e. $S$ is bounded. The reasoning holds for all $\alpha$

$(1)\Rightarrow (2)$ For any given $r$, consider $T=\{x\in\bR^n:f(x)\leq r\}$ bounded by assumption

\hspace{1.6cm}Hence $\exists\,\delta>0$ such that $T\subset B_{\delta}(0)$

\hspace{1.6cm}For all $x$ such that $||x||\geq \delta+1$, must have $x\notin T$, thus $f(x)>r$

\hspace{1.6cm}Letting $m=\delta+1$ and we are done
\end{proof}

\eg
Let $A\in\bR^{m\times n}$ be of rank $n$, then $f(x)=||Ax-b||$ with $b\in\bR^m$ is coercive

{\bf Trick} $f(x)=||Ax-b||\geq ||Ax||-||b||$ by the triangle inequality

Note $A^TA$ is P.S.D, in fact, p.d. because $A$ is full rank (Proposition 2.4.1)

\begin{align*}
    f(x)\geq ||Ax||-||b||&=\sqrt{(Ax)^T(Ax)}-||b||\\
    &=\sqrt{x^T(A^TA)x}-||b||\\
    &\geq \sqrt{\lambda_1||x||^2}-||b||\mbox{ by Proposition 2.3.3}\\
    &\geq \sqrt{\lambda_1}||x||-b
\end{align*}

So given any $r>0$, we have $f(x)\geq r$ when ever $||x||\geq \frac{r+||b||}{\sqrt{\lambda_1}}$




\end{document}
